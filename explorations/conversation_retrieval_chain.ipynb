{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "292a3c83-44a9-4426-bbec-f1a778d00d93",
      "metadata": {
        "id": "292a3c83-44a9-4426-bbec-f1a778d00d93"
      },
      "source": [
        "# Migrating from ConversationalRetrievalChain\n",
        "\n",
        "The [`ConversationalRetrievalChain`](https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html) was an all-in one way that combined retrieval-augmented generation with chat history, allowing you to \"chat with\" your documents.\n",
        "\n",
        "Advantages of switching to the LCEL implementation are similar to the [`RetrievalQA` migration guide](./retrieval_qa.ipynb):\n",
        "\n",
        "- Clearer internals. The `ConversationalRetrievalChain` chain hides an entire question rephrasing step which dereferences the initial query against the chat history.\n",
        "  - This means the class contains two sets of configurable prompts, LLMs, etc.\n",
        "- More easily return source documents.\n",
        "- Support for runnable methods like streaming and async operations.\n",
        "\n",
        "Here are equivalent implementations with custom prompts.\n",
        "We'll use the following ingestion code to load a [blog post by Lilian Weng](https://lilianweng.github.io/posts/2023-06-23-agent/) on autonomous agents into a local vector store:\n",
        "\n",
        "## Shared setup\n",
        "\n",
        "For both versions, we'll need to load the data with the `WebBaseLoader` document loader, split it with `RecursiveCharacterTextSplitter`, and add it to an in-memory `FAISS` vector store.\n",
        "\n",
        "We will also instantiate a chat model to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b99b47ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b99b47ec",
        "outputId": "9ceaa2bc-80ca-4c78-9181-f2cd933a55d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m938.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet langchain-community langchain langchain-openai faiss-cpu beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "717c8673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "717c8673",
        "outputId": "f6885d74-9e7c-4222-d38a-43bb5e9ba628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lQuY2wcTulP_"
      },
      "id": "lQuY2wcTulP_",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "44119498-5a98-4077-9e2f-c75500e7eace",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44119498-5a98-4077-9e2f-c75500e7eace",
        "outputId": "78cf768f-b09b-4b03-8b84-f7dde1dbfb65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "# Load docs\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
        "data = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "\n",
        "# Store splits\n",
        "vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bc06416",
      "metadata": {
        "id": "8bc06416"
      },
      "source": [
        "## Legacy\n",
        "\n",
        "<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8b471e7d-3ccb-4ab3-bc09-304c4b14a908",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b471e7d-3ccb-4ab3-bc09-304c4b14a908",
        "outputId": "df4508d3-d7cc-44f2-e7e9-0eb3f7b2859b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-598a7db33f5b>:41: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  convo_qa_chain(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What are autonomous agents?',\n",
              " 'chat_history': '',\n",
              " 'answer': 'Autonomous agents are empowered by LLM to handle complex scientific tasks independently, such as designing, planning, and executing experiments. These agents can browse the Internet, read documentation, execute code, and leverage other LLMs. Boiko et al. (2023) investigated LLM-powered autonomous agents for scientific discovery.'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "condense_question_template = \"\"\"\n",
        "Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "Follow Up Input: {question}\n",
        "Standalone question:\"\"\"\n",
        "\n",
        "condense_question_prompt = ChatPromptTemplate.from_template(condense_question_template)\n",
        "\n",
        "qa_template = \"\"\"\n",
        "You are an assistant for question-answering tasks.\n",
        "Use the following pieces of retrieved context to answer\n",
        "the question. If you don't know the answer, say that you\n",
        "don't know. Use three sentences maximum and keep the\n",
        "answer concise.\n",
        "\n",
        "Chat History:\n",
        "{chat_history}\n",
        "\n",
        "Other context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
        "\n",
        "convo_qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    vectorstore.as_retriever(),\n",
        "    condense_question_prompt=condense_question_prompt,\n",
        "    combine_docs_chain_kwargs={\n",
        "        \"prompt\": qa_prompt,\n",
        "    },\n",
        ")\n",
        "\n",
        "convo_qa_chain(\n",
        "    {\n",
        "        \"question\": \"What are autonomous agents?\",\n",
        "        \"chat_history\": \"\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a8a23c",
      "metadata": {
        "id": "43a8a23c"
      },
      "source": [
        "</details>\n",
        "\n",
        "## LCEL\n",
        "\n",
        "<details open>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "35657a13-ad67-4af1-b1f9-f58606ae43b4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35657a13-ad67-4af1-b1f9-f58606ae43b4",
        "outputId": "f2a81f25-bc4f-47e9-d793-5a86bb2b9457"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What are autonomous agents?',\n",
              " 'chat_history': [],\n",
              " 'context': [Document(id='64f848e1-c664-4f6a-86c8-0a0905e859a9', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Boiko et al. (2023) also looked into LLM-empowered agents for scientific discovery, to handle autonomous design, planning, and performance of complex scientific experiments. This agent can use tools to browse the Internet, read documentation, execute code, call robotics experimentation APIs and leverage other LLMs.\\nFor example, when requested to \"develop a novel anticancer drug\", the model came up with the following reasoning steps:'),\n",
              "  Document(id='9fcf1b60-cc76-44ba-8c1a-24d9652c7a7e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\"),\n",
              "  Document(id='89b3404a-0e9c-4f47-af6a-c51db8b79d56', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Weng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.'),\n",
              "  Document(id='f0ff8af9-7e5f-4869-98b1-0eb78968f0a0', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#')],\n",
              " 'answer': 'Autonomous agents are entities that can act independently to achieve specific goals with minimal human intervention. These agents can make decisions, plan activities, and adapt to changing environments autonomously. They can utilize various tools, technologies, and information to perform tasks without constant human direction.'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "condense_question_system_template = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "\n",
        "condense_question_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", condense_question_system_template),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm,\n",
        "    vectorstore.as_retriever(),\n",
        "    condense_question_prompt\n",
        ")\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "\n",
        "chain_result = convo_qa_chain.invoke(\n",
        "    {\n",
        "        \"input\": \"What are autonomous agents?\",\n",
        "        \"chat_history\": [],\n",
        "    }\n",
        ")\n",
        "\n",
        "chain_result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_chain(llm, vectorstore):\n",
        "    condense_question_system_template = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood \"\n",
        "        \"without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "\n",
        "    condense_question_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", condense_question_system_template),\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm,\n",
        "        vectorstore.as_retriever(),\n",
        "        condense_question_prompt\n",
        "    )\n",
        "\n",
        "    system_prompt = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of retrieved context to answer \"\n",
        "        \"the question. If you don't know the answer, say that you \"\n",
        "        \"don't know. Use three sentences maximum and keep the \"\n",
        "        \"answer concise.\"\n",
        "        \"\\n\\n\"\n",
        "        \"{context}\"\n",
        "    )\n",
        "\n",
        "    qa_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", system_prompt),\n",
        "            (\"placeholder\", \"{chat_history}\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    convo_qa_chain = create_retrieval_chain(history_aware_retriever, qa_chain)\n",
        "    return convo_qa_chain\n",
        "\n",
        "def chat_with_retrieval_chain(input, chat_history):\n",
        "    result = convo_qa_chain.invoke(\n",
        "        {\n",
        "            \"input\": input,\n",
        "            \"chat_history\": chat_history,\n",
        "        }\n",
        "    )\n",
        "    chat_history.extend([\n",
        "        (\"human\", input),\n",
        "        (\"ai\", result[\"answer\"])\n",
        "    ])\n",
        "    return result[\"answer\"], chat_history\n",
        "\n",
        "def chat_with_history_aware(input, chat_history):\n",
        "    system_template = (\n",
        "        \"You are an assistant for question-answering tasks. \"\n",
        "        \"Use the following pieces of chat history as contexts to answer \"\n",
        "        \"the question. If you don't know the answer, say that you \"\n",
        "        \"don't know. Use three sentences maximum and keep the \"\n",
        "        \"answer concise.\"\n",
        "        \"chat_history: {chat_history}\"\n",
        "        \"query: {query}\"\n",
        "    )\n",
        "    query_classification_prompt = ChatPromptTemplate.from_template(system_template)\n",
        "    chain = query_classification_prompt | llm\n",
        "    result = chain.invoke({\n",
        "        \"query\": input,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "    chat_history.extend([\n",
        "        (\"human\", input),\n",
        "        (\"ai\", result.content)\n",
        "    ])\n",
        "    return result.content, chat_history\n",
        "\n",
        "def chat(input, chat_history):\n",
        "    query_classification_template = (\n",
        "        \"your task is to determine whether the query needs to retrieve documents or not and whether needs chat_history as context or not.\"\n",
        "        \"possible answers are only 3 of the following:\"\n",
        "        \"  'YES': the query demands retrieving documents\"\n",
        "        \"  'NO': the query demands no additional contexts\"\n",
        "        \"  'ONLY_CHAT_HISTORY': the query demands only to aware of history of the chat\"\n",
        "        \"query: {query}\"\n",
        "    )\n",
        "    query_classification_prompt = ChatPromptTemplate.from_template(query_classification_template)\n",
        "    chain = query_classification_prompt | llm\n",
        "    result = chain.invoke({\n",
        "        \"query\": input\n",
        "    })\n",
        "    ai_msg = result.content\n",
        "    if \"yes\" in ai_msg.lower():\n",
        "        print(\"The query demands retrieving documents\\n\")\n",
        "        return chat_with_retrieval_chain(input, chat_history)\n",
        "    elif \"only_chat_history\" in ai_msg.lower():\n",
        "        print(\"The query demands only to aware of history of the chat\\n\")\n",
        "        return chat_with_history_aware(input, chat_history)\n",
        "    elif \"no\" in ai_msg.lower():\n",
        "        print(\"The query demands no additional contexts\\n\")\n",
        "        system_template = (\n",
        "            \"You are an assistant for question-answering tasks. \"\n",
        "            \"If you don't know the answer, say that you \"\n",
        "            \"don't know. Use three sentences maximum and keep the \"\n",
        "            \"answer concise.\"\n",
        "            \"query: {query}\"\n",
        "        )\n",
        "        query_classification_prompt = ChatPromptTemplate.from_template(system_template)\n",
        "        chain = query_classification_prompt | llm\n",
        "        result = chain.invoke({\n",
        "            \"query\": input\n",
        "        })\n",
        "        chat_history.extend([\n",
        "            (\"human\", input),\n",
        "            (\"ai\", result.content)\n",
        "        ])\n",
        "        return result.content, chat_history\n",
        "    else:\n",
        "        print(\"Unintended answer\")\n",
        "        print(ai_msg)"
      ],
      "metadata": {
        "id": "1VPSQgnou3Od"
      },
      "id": "1VPSQgnou3Od",
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "convo_qa_chain = build_chain(llm, vectorstore)"
      ],
      "metadata": {
        "id": "R6HrHHUk3vaB"
      },
      "id": "R6HrHHUk3vaB",
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer, chat_history = chat(\n",
        "    \"According to the document, what are autonomous agents?\",\n",
        "    chat_history = [],\n",
        ")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t60uJII7vcVd",
        "outputId": "87617fca-b44f-4b7a-ddb3-55f8c49d0a4a"
      },
      "id": "t60uJII7vcVd",
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query demands retrieving documents\n",
            "\n",
            "Autonomous agents are LLM-empowered agents capable of handling autonomous design, planning, and performance of complex scientific experiments. These agents can browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs for tasks like developing novel anticancer drugs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y0zg_gZwedD",
        "outputId": "704d7131-d5c8-408a-c5f7-a9c7c4b9c991"
      },
      "id": "1Y0zg_gZwedD",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('human', 'According to the document, what are autonomous agents?'),\n",
              " ('ai',\n",
              "  'Autonomous agents are LLM-empowered agents capable of handling autonomous design, planning, and performance of complex scientific experiments. These agents can browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs for tasks like developing novel anticancer drugs.')]"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_2, chat_history_2 = chat(\n",
        "    \"Elaborate to me our past conversation\",\n",
        "    chat_history = chat_history,\n",
        ")\n",
        "print(answer_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shayg-H_zd0n",
        "outputId": "e76a816a-9267-42dc-d0c3-a66ac66d6fdf"
      },
      "id": "Shayg-H_zd0n",
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query demands only to aware of history of the chat\n",
            "\n",
            "In our previous conversation, we discussed autonomous agents being LLM-empowered entities capable of autonomously designing, planning, and executing complex scientific experiments. These agents can access the Internet, read documentation, run code, interact with robotics APIs, and collaborate with other LLMs for tasks such as creating new anticancer drugs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0QvXm8J3RSA",
        "outputId": "96d2d254-caa7-412a-ee71-fed54046a2b8"
      },
      "id": "X0QvXm8J3RSA",
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('human', 'According to the document, what are autonomous agents?'),\n",
              " ('ai',\n",
              "  'Autonomous agents are LLM-empowered agents capable of handling autonomous design, planning, and performance of complex scientific experiments. These agents can browse the Internet, read documentation, execute code, call robotics experimentation APIs, and leverage other LLMs for tasks like developing novel anticancer drugs.'),\n",
              " ('human', 'Elaborate to me our past conversation'),\n",
              " ('ai',\n",
              "  'In our previous conversation, we discussed autonomous agents being LLM-empowered entities capable of autonomously designing, planning, and executing complex scientific experiments. These agents can access the Internet, read documentation, run code, interact with robotics APIs, and collaborate with other LLMs for tasks such as creating new anticancer drugs.')]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer_3, chat_history_3 = chat(\n",
        "    \"What is my first question to you?\",\n",
        "    chat_history = chat_history,\n",
        ")\n",
        "print(answer_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rSGiuWM-xBR",
        "outputId": "3a407faa-2d24-46ce-8e7c-6749c36dab14"
      },
      "id": "_rSGiuWM-xBR",
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The query demands only to aware of history of the chat\n",
            "\n",
            "Your first question to me was, \"According to the document, what are autonomous agents?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Retrieval Chain"
      ],
      "metadata": {
        "id": "t8l4tRoKvgmc"
      },
      "id": "t8l4tRoKvgmc"
    },
    {
      "cell_type": "code",
      "source": [
        "create_retrieval_chain.from_llm_and_retriever(llm, vectorstore.as_retriever())"
      ],
      "metadata": {
        "id": "8P7Ovyovvh_7"
      },
      "id": "8P7Ovyovvh_7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b2717810",
      "metadata": {
        "id": "b2717810"
      },
      "source": [
        "</details>\n",
        "\n",
        "## Next steps\n",
        "\n",
        "You've now seen how to migrate existing usage of some legacy chains to LCEL.\n",
        "\n",
        "Next, check out the [LCEL conceptual docs](/docs/concepts/lcel) for more background information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bfc38bd-0ff8-40ee-83a3-9d7553364fd7",
      "metadata": {
        "id": "7bfc38bd-0ff8-40ee-83a3-9d7553364fd7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}